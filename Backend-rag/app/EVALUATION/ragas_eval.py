# app/chains/ragas_eval.py

from ragas.metrics import faithfulness, answer_relevancy, context_recall, context_precision
from ragas import evaluate
from datasets import Dataset

# Initialize evaluator once (reuse for multiple queries)
# evaluator = RagasEvaluator()

def evaluate_answer(question: str, answer: str, context_docs: list[str]) -> dict:
    """
    Evaluate a RAG answer using RAGAS metrics.

    Args:
        question (str): The user question.
        answer (str): The answer generated by your RAG chain.
        context_docs (list[str]): List of context documents used to generate the answer.

    Returns:
        dict: Evaluation results with faithfulness, answer_relevancy, context_recall, context_precision scores
    """
    # Prepare data in the format RAGAS expects
    eval_data = {
        "question": [question],
        "answer": [answer],
        "contexts": [context_docs]
    }
    
    dataset = Dataset.from_dict(eval_data)
    
    # Define metrics to evaluate
    metrics = [
        faithfulness,
        answer_relevancy,
        context_recall,
        context_precision
    ]
    
    # Run evaluation
    results = evaluate(dataset, metrics=metrics)
    
    return results
